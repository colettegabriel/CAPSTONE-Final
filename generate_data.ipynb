{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'query_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4729963bbdb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mquery_code\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_process\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfile_data_att\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'query_code'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 16 16:39:52 2019\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import query_code as q\n",
    "import data_process as process\n",
    "import file_data_att as fd\n",
    "from typing import Dict\n",
    "import settings\n",
    "from queries_PIM import _attr_query, sys2_attr_values, sys2_attr_query\n",
    "import time\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_category(df):\n",
    "    \"\"\"compare data colected from matching file (match_df) with sys1 and sys2 data pulls and create a column to tell analysts\n",
    "    whether attributes from the two systems have been matched\"\"\"\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        if (row.Index, row.alt_sys1_name) == (row.Index, row.alt_sys2_name):\n",
    "            df.at[row.Index,'Matching'] = 'Match'\n",
    "        elif process.isBlank(row.sys1_Attribute_Name) == False:\n",
    "            if process.isBlank(row.sys2_Attribute_Name) == True:\n",
    "                df.at[row.Index,'Matching'] = 'sys1 only'\n",
    "        elif process.isBlank(row.sys1_Attribute_Name) == True:\n",
    "            if process.isBlank(row.sys2_Attribute_Name) == False:\n",
    "                df.at[row.Index,'Matching'] = 'sys2 only'\n",
    "             \n",
    "    return df\n",
    "\n",
    "\n",
    "def sys2_process(count, order, node, sys2_dict: Dict, k):\n",
    "    \"\"\"if sys2 node has not been previously processed (in sys2_dict), process and add it to the dictionary\"\"\"\n",
    "    sys2_sample_vals = pd.DataFrame()\n",
    "    sys2_att_vals = pd.DataFrame()\n",
    "\n",
    "    sys2_df = q.sys2_atts(sys2_attr_query, node, 'tax.id')  #tprod.\"categoryId\"')  #get sys2 attribute values for each sys2_l3 node\\\n",
    "    \n",
    "    if sys2_df.empty==False:\n",
    "        sys2_att_vals, sys2_sample_vals = q.sys2_values(sys2_attr_values, node, 'tax.id') #sys2_values exports a list of --all-- normalized values and sample_values\n",
    "        \n",
    "        if sys2_att_vals.empty==False:\n",
    "            sys2_sample_vals = sys2_sample_vals.rename(columns={'Normalized Value': 'sys2 Attribute Sample Values'})\n",
    "            sys2_df = pd.merge(sys2_df, sys2_sample_vals, on=['sys2_Attribute_Name'])  #add t0p 5 normalized values to report\n",
    "            sys2_df = pd.merge(sys2_df, sys2_att_vals, on=['sys2_Attr_ID'])  #add t0p 5 normalized values to report\n",
    "        else:\n",
    "            if count == 1:\n",
    "                order = 'alt'\n",
    "\n",
    "        sys2_df = sys2_df.drop_duplicates(subset='sys2_Attr_ID')  #sys2 attribute IDs are unique, so no need to group by pim node before getting unique\n",
    "        sys2_df['alt_sys2_name'] = process.process_att(sys2_df['sys2_Attribute_Name'])  #prep att name for merge\n",
    "        \n",
    "        sys2_dict[node] = sys2_df #store the processed df in dict for future reference\n",
    " \n",
    "    else:\n",
    "        print('{} EMPTY DATAFRAME'.format(node))    \n",
    "        \n",
    "    return sys2_dict, sys2_df, order\n",
    "\n",
    "\n",
    "def sys1_assign_nodes (sys1_df, sys2_df):\n",
    "    \"\"\"assign sys2 node data to sys1 columns\"\"\"\n",
    "    \n",
    "    att_list = []\n",
    "    \n",
    "    node_ID = sys2_df['sys2_Node_ID']\n",
    "    cat_ID = sys2_df['sys2_Category_ID']\n",
    "    cat_name = sys2_df['sys2_Category_Name']\n",
    "    node_name = sys2_df['sys2_Node_Name']\n",
    "    pim_path = sys2_df['sys2_PIM_Path']\n",
    "\n",
    "    att_list = sys1_df['sys1_Attribute_Name'].unique()\n",
    "    \n",
    "    for att in att_list:\n",
    "        sys1_df.loc[sys1_df['sys1_Attribute_Name'] == att, 'sys2_Node_ID'] = node_ID\n",
    "        sys1_df.loc[sys1_df['sys1_Attribute_Name'] == att, 'sys2_Category_ID'] = cat_ID\n",
    "        sys1_df.loc[sys1_df['sys1_Attribute_Name'] == att, 'sys2_Category_Name'] = cat_name\n",
    "        sys1_df.loc[sys1_df['sys1_Attribute_Name'] == att, 'sys2_Node_Name'] = node_name\n",
    "        sys1_df.loc[sys1_df['sys1_Attribute_Name'] == att, 'sys2_PIM_Path'] = pim_path\n",
    "    \n",
    "    return sys1_df\n",
    "\n",
    "\n",
    "def sys2_assign_nodes (sys1_df, sys2_df):\n",
    "    \"\"\"assign sys1 node data to sys2 columns\"\"\"\n",
    "    \n",
    "    att_list = []\n",
    "    \n",
    "    blue = sys1_df['sys1 Blue Path'].unique()\n",
    "    seg_ID = sys1_df['Segment_ID'].unique()\n",
    "    seg_name = sys1_df['Segment_Name'].unique()\n",
    "    fam_ID = sys1_df['Family_ID'].unique()\n",
    "    fam_name = sys1_df['Family_Name'].unique()\n",
    "    cat_ID = sys1_df['Category_ID'].unique()\n",
    "    cat_name = sys1_df['Category_Name'].unique()\n",
    "    \n",
    "    att_list = sys2_df['sys2_Attribute_Name'].unique()\n",
    "    \n",
    "    for att in att_list:\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'Category_ID'] = cat_ID\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'sys1 Blue Path'] = blue\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'Segment_ID'] = seg_ID\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'Segment_Name'] = seg_name\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'Family_ID'] = fam_ID\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'Family_Name'] = fam_name\n",
    "        sys2_df.loc[sys2_df['sys2_Attribute_Name'] == att, 'Category_Name'] = cat_name\n",
    "    \n",
    "    return sys2_df\n",
    "\n",
    "\n",
    "def sys1_process(sys1_df, sys1_sample, sys1_all, sys2_dict: Dict, k):\n",
    "    \"\"\"create a list of sys1 skus, run through through the sys2_skus query and pull sys2 attribute data if skus are present\n",
    "        concat both dataframs and join them on matching attribute names\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    order = 'normal'\n",
    "    count = 1\n",
    "        \n",
    "    cat_name = sys1_df['Category_Name'].unique()\n",
    "    cat_name = list(cat_name)\n",
    "    cat_name = cat_name.pop()\n",
    "    print('cat name = {} {}'.format(k, cat_name))\n",
    "    \n",
    "    sys1_skus = sys1_df.drop_duplicates(subset='sys1_SKU')  #create list of unique sys1 skus that feed into sys2 query\n",
    "    sys1_sku_count = len(sys1_skus)\n",
    "    print('sys1 sku count = ', sys1_sku_count)\n",
    "    sys1_df = sys1_df.drop_duplicates(subset=['Category_ID', 'sys1_Attr_ID'])  #group by Category_ID and attribute name and keep unique\n",
    "    sys1_df['sys1 Blue Path'] = sys1_df['Segment_Name'] + ' > ' + sys1_df['Family_Name'] + \\\n",
    "                                                        ' > ' + sys1_df['Category_Name']\n",
    "\n",
    "    sys1_df = sys1_df.drop(['sys1_SKU', 'sys1_Attribute_Value'], axis=1) #remove unneeded columns\n",
    "    sys1_df = pd.merge(sys1_df, sys1_sample, on=['sys1_Attribute_Name'])\n",
    "    sys1_df = pd.merge(sys1_df, sys1_all, on=['sys1_Attr_ID'])\n",
    "    \n",
    "    sys1_df['alt_sys1_name'] = process.process_att(sys1_df['sys1_Attribute_Name'])  #prep att name for merge\n",
    "    #sys1_df.to_csv (\"F:/CGabriel/sys1_Shorties/OUTPUT/sys1_test.csv\")\n",
    "    \n",
    "    sys2_skus = q.sys2_skus(sys1_skus) #get sys2 sku list to determine pim nodes to pull\n",
    "    if sys2_skus.empty==False:\n",
    "        #create a dictionary of the unique sys2 nodes that corresponde to the sys1 node\n",
    "        sys2_l3 = sys2_skus['sys2_Node_ID'].unique()  #create list of pim nodes to pull\n",
    "        print('sys2 L3s ', sys2_l3)\n",
    "        \n",
    "        for node in sys2_l3:\n",
    "            if node in sys2_dict:\n",
    "                sys2_df = sys2_dict[node]\n",
    "            else:\n",
    "                sys2_dict, sys2_df, order = sys2_process(count, order, node, sys2_dict, k)\n",
    "            \n",
    "            if sys2_df.empty==False:\n",
    "                node_name = sys2_df['sys2_Node_Name'].unique()\n",
    "                node_name = list(node_name)\n",
    "                node_name = node_name.pop()\n",
    "                print('node name = {} {}'.format(node, node_name))\n",
    "                #add correlating sys1 and sys2 data to opposite dataframes\n",
    "                sys1_df = sys1_assign_nodes(sys1_df, sys2_df)\n",
    "                sys2_df = sys2_assign_nodes(sys1_df, sys2_df)\n",
    " \n",
    "                skus = sys2_skus[sys2_skus['sys2_Node_ID'] == node]\n",
    "                temp_df = pd.merge(sys1_df, sys2_df, left_on=['sys1_Attribute_Name', 'Category_ID', 'sys2_Node_ID', 'sys2_Category_ID', \\\n",
    "                                                                   'sys2_Category_Name', 'sys2_Node_Name', 'sys2_PIM_Path', 'sys1 Blue Path', \\\n",
    "                                                                   'Segment_ID', 'Segment_Name', 'Family_ID', 'Family_Name', 'Category_Name'], \n",
    "                                                right_on=['sys2_Attribute_Name', 'Category_ID', 'sys2_Node_ID', 'sys2_Category_ID', \\\n",
    "                                                          'sys2_Category_Name', 'sys2_Node_Name', 'sys2_PIM_Path', 'sys1 Blue Path', \\\n",
    "                                                          'Segment_ID', 'Segment_Name', 'Family_ID', 'Family_Name', 'Category_Name'], how='outer')\n",
    "                temp_df = match_category(temp_df) #compare sys1 and sys2 atts and create column to say whether they match \n",
    "                temp_df['sys1_sku_count'] = sys1_sku_count\n",
    "                temp_df['sys2_sku_count'] = len(skus)#temp_skus['sys2_SKU']\n",
    "                temp_df['sys1-sys2 Terminal Node Mapping'] = cat_name+' -- '+node_name\n",
    "                temp_df['sys2/sys1 SKU Counts'] = temp_df['sys2_sku_count'].map(str)+' / '+temp_df['sys1_sku_count'].map(str)\n",
    "                \n",
    "                df = pd.concat([df, temp_df], axis=0, sort=False) #add prepped df for this sys2 node to the final df\n",
    "            else:\n",
    "                print('sys2 Node {} EMPTY DATAFRAME'.format(node))\n",
    "            count += 1\n",
    "\n",
    "    else:\n",
    "        print('No sys2 SKUs for sys1 node {}'.format(k))\n",
    "        \n",
    "    return df, sys2_dict, order  #where sys2_att_temp is the list of all normalized values for sys2 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine SKU or node search\n",
    "search_level = 'cat.CATEGORY_ID'\n",
    "\n",
    "sys2_df = pd.DataFrame()\n",
    "sys1_df = pd.DataFrame()\n",
    "sys1_skus = pd.DataFrame()\n",
    "\n",
    "attribute_df = pd.DataFrame()\n",
    "sys1_att_vals = pd.DataFrame()\n",
    "sys1_sample_vals = pd.DataFrame()\n",
    "sys2_att_vals = pd.DataFrame()\n",
    "sys2_dict = dict()\n",
    "\n",
    "column_order = 'normal'\n",
    "count = 1\n",
    "\n",
    "data_type = fd.search_type()\n",
    "\n",
    "if data_type == 'sys1_query':\n",
    "    search_level = fd.blue_search_level()\n",
    "    \n",
    "search_data = fd.data_in(data_type, settings.directory_name)\n",
    "\n",
    "start_time = time.time()\n",
    "print('working...')\n",
    "\n",
    "if data_type == 'sys1_query':\n",
    "    if search_level == 'cat.CATEGORY_ID':\n",
    "        for k in search_data:\n",
    "            sys1_df = q.gcom.sys1_q(sys1_attr_query, search_level, k)\n",
    "            if sys1_df.empty == False:\n",
    "                sys1_att_vals, sys1_sample_vals = q.sys1_values(sys1_df)\n",
    "                sys1_sample_vals = sys1_sample_vals.rename(columns={'sys1_Attribute_Value': 'sys1 Attribute Sample Values'})\n",
    "                sys1_att_vals = sys1_att_vals.rename(columns={'sys1_Attribute_Value': 'sys1 ALL Values'})\n",
    "                temp_df, sys2_dict, order = sys1_process(sys1_df, sys1_sample_vals, sys1_att_vals, sys2_dict, k)\n",
    "                if order == 'alt':\n",
    "                    column_order = 'alt'\n",
    "                attribute_df = pd.concat([attribute_df, temp_df], axis=0, sort=False)\n",
    "                print ('sys1 ', k)\n",
    "            else:\n",
    "                print('No attribute data')\n",
    "    else:\n",
    "        for k in search_data:\n",
    "            print('K = ', k)\n",
    "            sys1_skus = q.sys1_nodes(k, search_level)\n",
    "            #sys1_skus = pd.concat([sys1_skus, temp_df], axis=0, sort=False)\n",
    "            sys1_l3 = sys1_skus['Category_ID'].unique()  #create list of pim nodes to pull\n",
    "            print('sys1 L3s = ', sys1_l3)\n",
    "            for j in sys1_l3:\n",
    "                sys1_df = q.gcom.sys1_q(sys1_attr_query, 'cat.CATEGORY_ID', j)\n",
    "                if sys1_df.empty == False:\n",
    "                    sys1_att_vals, sys1_sample_vals = q.sys1_values(sys1_df)\n",
    "                    sys1_sample_vals = sys1_sample_vals.rename(columns={'sys1_Attribute_Value': 'sys1 Attribute Sample Values'})\n",
    "                    temp_df, sys2_dict, order = sys1_process(sys1_df, sys1_sample_vals, sys1_att_vals, sys2_dict, j)\n",
    "                    if order == 'alt':\n",
    "                        column_order = 'alt'\n",
    "                    attribute_df = pd.concat([attribute_df, temp_df], axis=0, sort=False)\n",
    "                    print ('sys1 ', j)\n",
    "                else:\n",
    "                    print('sys1 node {} All SKUs are R4, R9, or discontinued'.format(j)) \n",
    "            print(\"--- {} seconds ---\".format(round(time.time() - start_time, 2)))\n",
    "\n",
    "\n",
    "attribute_df = attribute_df.drop(['Count'], axis=1)\n",
    "\n",
    "\n",
    "fd.attribute_match_data_out(settings.directory_name, attribute_df, column_order, search_level)            \n",
    "\n",
    "print(\"--- {} seconds ---\".format(round(time.time() - start_time, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
